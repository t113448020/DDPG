{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e33de84",
   "metadata": {},
   "source": [
    "\n",
    "# DDPG 演算法實作（Pendulum-v1 環境）- Google Colab 版\n",
    "\n",
    "本 Notebook 整合了完整的 DDPG 類別、訓練流程、測試程式，並支援 Gym 新版 API。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install tensorlayer==2.2.3 tensorflow-probability==0.6.0 gym pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13478380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorlayer as tl\n",
    "\n",
    "ENV_NAME = 'Pendulum-v1'\n",
    "RANDOMSEED = 1\n",
    "\n",
    "LR_A = 0.001\n",
    "LR_C = 0.002\n",
    "GAMMA = 0.9\n",
    "TAU = 0.01\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_EPISODES = 200\n",
    "MAX_EP_STEPS = 200\n",
    "TEST_PER_EPISODES = 10\n",
    "VAR = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset(seed=RANDOMSEED)\n",
    "np.random.seed(RANDOMSEED)\n",
    "tf.random.set_seed(RANDOMSEED)\n",
    "\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "a_bound = env.action_space.high\n",
    "\n",
    "print('s_dim:', s_dim)\n",
    "print('a_dim:', a_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound\n",
    "\n",
    "        W_init = tf.random_normal_initializer(mean=0, stddev=0.3)\n",
    "        b_init = tf.constant_initializer(0.1)\n",
    "\n",
    "        def get_actor(input_state_shape, name=''):\n",
    "            inputs = tl.layers.Input(input_state_shape, name='A_input')\n",
    "            x = tl.layers.Dense(n_units=30, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='A_l1')(inputs)\n",
    "            x = tl.layers.Dense(n_units=a_dim, act=tf.nn.tanh, W_init=W_init, b_init=b_init, name='A_a')(x)\n",
    "            x = tl.layers.Lambda(lambda x: a_bound * x)(x)\n",
    "            return tl.models.Model(inputs=inputs, outputs=x, name='Actor' + name)\n",
    "\n",
    "        def get_critic(input_state_shape, input_action_shape, name=''):\n",
    "            s = tl.layers.Input(input_state_shape, name='C_s_input')\n",
    "            a = tl.layers.Input(input_action_shape, name='C_a_input')\n",
    "            x = tl.layers.Concat(1)([s, a])\n",
    "            x = tl.layers.Dense(n_units=60, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='C_l1')(x)\n",
    "            x = tl.layers.Dense(n_units=1, W_init=W_init, b_init=b_init, name='C_out')(x)\n",
    "            return tl.models.Model(inputs=[s, a], outputs=x, name='Critic' + name)\n",
    "\n",
    "        self.actor = get_actor([None, s_dim])\n",
    "        self.critic = get_critic([None, s_dim], [None, a_dim])\n",
    "        self.actor.train()\n",
    "        self.critic.train()\n",
    "\n",
    "        def copy_para(from_model, to_model):\n",
    "            for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):\n",
    "                j.assign(i)\n",
    "\n",
    "        self.actor_target = get_actor([None, s_dim], name='_target')\n",
    "        self.critic_target = get_critic([None, s_dim], [None, a_dim], name='_target')\n",
    "        copy_para(self.actor, self.actor_target)\n",
    "        copy_para(self.critic, self.critic_target)\n",
    "        self.actor_target.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "        self.ema = tf.train.ExponentialMovingAverage(decay=1 - TAU)\n",
    "        self.actor_opt = tf.optimizers.Adam(LR_A)\n",
    "        self.critic_opt = tf.optimizers.Adam(LR_C)\n",
    "\n",
    "    def ema_update(self):\n",
    "        paras = self.actor.trainable_weights + self.critic.trainable_weights\n",
    "        self.ema.apply(paras)\n",
    "        for i, j in zip(self.actor_target.trainable_weights + self.critic_target.trainable_weights, paras):\n",
    "            i.assign(self.ema.average(j))\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        return self.actor(np.array([s], dtype=np.float32))[0]\n",
    "\n",
    "    def learn(self):\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, self.s_dim:self.s_dim + self.a_dim]\n",
    "        br = bt[:, -self.s_dim - 1:-self.s_dim]\n",
    "        bs_ = bt[:, -self.s_dim:]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            a_ = self.actor_target(bs_)\n",
    "            q_ = self.critic_target([bs_, a_])\n",
    "            y = br + GAMMA * q_\n",
    "            q = self.critic([bs, ba])\n",
    "            td_error = tf.losses.mean_squared_error(y, q)\n",
    "        c_grads = tape.gradient(td_error, self.critic.trainable_weights)\n",
    "        self.critic_opt.apply_gradients(zip(c_grads, self.critic.trainable_weights))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            a = self.actor(bs)\n",
    "            q = self.critic([bs, a])\n",
    "            a_loss = -tf.reduce_mean(q)\n",
    "        a_grads = tape.gradient(a_loss, self.actor.trainable_weights)\n",
    "        self.actor_opt.apply_gradients(zip(a_grads, self.actor.trainable_weights))\n",
    "\n",
    "        self.ema_update()\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        s = s.astype(np.float32)\n",
    "        s_ = s_.astype(np.float32)\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "    def save_ckpt(self):\n",
    "        if not os.path.exists('model'):\n",
    "            os.makedirs('model')\n",
    "        tl.files.save_weights_to_hdf5('model/ddpg_actor.hdf5', self.actor)\n",
    "        tl.files.save_weights_to_hdf5('model/ddpg_actor_target.hdf5', self.actor_target)\n",
    "        tl.files.save_weights_to_hdf5('model/ddpg_critic.hdf5', self.critic)\n",
    "        tl.files.save_weights_to_hdf5('model/ddpg_critic_target.hdf5', self.critic_target)\n",
    "\n",
    "    def load_ckpt(self):\n",
    "        tl.files.load_hdf5_to_weights_in_order('model/ddpg_actor.hdf5', self.actor)\n",
    "        tl.files.load_hdf5_to_weights_in_order('model/ddpg_actor_target.hdf5', self.actor_target)\n",
    "        tl.files.load_hdf5_to_weights_in_order('model/ddpg_critic.hdf5', self.critic)\n",
    "        tl.files.load_hdf5_to_weights_in_order('model/ddpg_critic_target.hdf5', self.critic_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG(a_dim, s_dim, a_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b71c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reward_buffer = []\n",
    "t0 = time.time()\n",
    "for i in range(MAX_EPISODES):\n",
    "    s, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        a = ddpg.choose_action(s)\n",
    "        a = np.clip(np.random.normal(a, VAR), -2, 2)\n",
    "        s_, r, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        ddpg.store_transition(s, a, r / 10, s_)\n",
    "        if ddpg.pointer > MEMORY_CAPACITY:\n",
    "            ddpg.learn()\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if i and not i % TEST_PER_EPISODES:\n",
    "        print(f\"Episode: {i}, Reward: {ep_reward:.2f}\")\n",
    "        reward_buffer.append(ep_reward)\n",
    "\n",
    "ddpg.save_ckpt()\n",
    "print(\"Training done in %.2f seconds.\" % (time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8909938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ddpg.load_ckpt()\n",
    "\n",
    "for episode in range(3):\n",
    "    s, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    for step in range(MAX_EP_STEPS):\n",
    "        a = ddpg.choose_action(s)\n",
    "        s, r, terminated, truncated, _ = env.step(a)\n",
    "        ep_reward += r\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    print(f\"[測試] Episode {episode + 1} reward: {ep_reward:.2f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
